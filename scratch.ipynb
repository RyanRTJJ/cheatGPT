{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting it all together\n",
    "# https://www.google.com/imgres?imgurl=https%3A%2F%2Fpbs.twimg.com%2Ftweet_video_thumb%2FFp2jXj3X0AAmbMP.jpg&imgrefurl=https%3A%2F%2Ftwitter.com%2Fpenguins%2Fstatus%2F1629638080981348357&tbnid=NUQAx98mbNyHeM&vet=12ahUKEwiS5rSJ_MD9AhWMPUQIHUTIBVoQMygEegUIARCeAQ..i&docid=EbDebVzkPJSurM&w=498&h=280&q=oh%20yeah%20it%27s%20all%20coming%20together&client=safari&ved=2ahUKEwiS5rSJ_MD9AhWMPUQIHUTIBVoQMygEegUIARCeAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreloads modules when they are changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# import components\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgenerators\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mGPT2\u001b[39;00m \u001b[39mimport\u001b[39;00m GPT2\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdiscriminators\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mBERT\u001b[39;00m \u001b[39mimport\u001b[39;00m BERT\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfitness_functions\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mTrivialFitnessFunction\u001b[39;00m \u001b[39mimport\u001b[39;00m TrivialFitnessFunction\n",
      "File \u001b[0;32m~/Documents/classes/CS 224N/cheatGPT/generators/GPT2.py:9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mGPT2-based generator\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39msubclass of general Generator class\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39muses GPT2-large from HuggingFace\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mhttps://huggingface.co/gpt2\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# imports superclass definition\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mGenerator\u001b[39;00m \u001b[39mimport\u001b[39;00m Generator\n\u001b[1;32m     11\u001b[0m \u001b[39m# imports huggingface infrastructure\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m# from transformers import GPT2Tokenizer, GPT2 LMHeadModel\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Generator'"
     ]
    }
   ],
   "source": [
    "# import components\n",
    "from generators.GPT2 import GPT2\n",
    "from discriminators.BERT import BERT\n",
    "from fitness_functions.TrivialFitnessFunction import TrivialFitnessFunction\n",
    "from utility_function import u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base-openai-detector were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/jstav/miniconda3/envs/proj/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:89: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# defines components\n",
    "generator = GPT2()\n",
    "discriminator = BERT()\n",
    "fitness_function = TrivialFitnessFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mThe quick brown fox jumps over the lazy dog.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m num_samples \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m----> 4\u001b[0m utility \u001b[39m=\u001b[39m u(prompt, generator, discriminator, fitness_function, num_samples)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(utility)\n",
      "File \u001b[0;32m~/Documents/classes/CS 224N/cheatGPT/utility_function.py:40\u001b[0m, in \u001b[0;36mu\u001b[0;34m(p, L, D, f, s)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mu\u001b[39m(p, L, D, f, s \u001b[39m=\u001b[39m synthesize):\n\u001b[1;32m     39\u001b[0m     Lp_batch \u001b[39m=\u001b[39m L\u001b[39m.\u001b[39mgenerate_batch(p, NUM_SAMPLES)\n\u001b[0;32m---> 40\u001b[0m     DLp_batch \u001b[39m=\u001b[39m D\u001b[39m.\u001b[39;49mdiscriminate_batch(Lp_batch)\n\u001b[1;32m     41\u001b[0m     fLp_batch \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mevaluate_batch(p, Lp_batch)\n\u001b[1;32m     43\u001b[0m     \u001b[39m# synthesize D, f scores\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/classes/CS 224N/cheatGPT/discriminators/BERT.py:43\u001b[0m, in \u001b[0;36mBERT.discriminate_batch\u001b[0;34m(self, passages)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdiscriminate_batch\u001b[39m(\u001b[39mself\u001b[39m, passages):\n\u001b[1;32m     42\u001b[0m     classifications \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(passages)\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m [classification[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m classification \u001b[39min\u001b[39;00m classifications]\n",
      "File \u001b[0;32m~/Documents/classes/CS 224N/cheatGPT/discriminators/BERT.py:43\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdiscriminate_batch\u001b[39m(\u001b[39mself\u001b[39m, passages):\n\u001b[1;32m     42\u001b[0m     classifications \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(passages)\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m [classification[\u001b[39m0\u001b[39;49m][\u001b[39m1\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m classification \u001b[39min\u001b[39;00m classifications]\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "# computes u given prompt\n",
    "prompt = \"The quick brown fox jumps over the lazy dog.\"\n",
    "num_samples = 1\n",
    "utility = u(prompt, generator, discriminator, fitness_function, num_samples)\n",
    "print(utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%reload` not found.\n"
     ]
    }
   ],
   "source": [
    "%reload discriminators\n",
    "import discriminators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'discriminators' has no attribute 'BERT'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m discriminators\u001b[39m.\u001b[39;49mBERT\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'discriminators' has no attribute 'BERT'"
     ]
    }
   ],
   "source": [
    "discriminators.BERT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e57984651113d8021af029180ed43ae14a5046115fd4444b2287457b0eb0f73c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
