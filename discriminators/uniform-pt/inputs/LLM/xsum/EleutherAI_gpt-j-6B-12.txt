 users block offensive content, or report it to the authorities. The draft code also requires companies to regularly inform users on an opt-in basis of the impact of their communications and how to remove such content.

Facebook and Google have already pledged to follow such guidelines.

But, a review of hate speech on social media reveals that such companies continue to publish offensive posts and promote far-right content while ignoring legitimate, leftist voices. This is despite the fact that they are aware of the problem for at least a decade.

In 2016, for instance, Facebook said it would allow users to flag potentially unlawful material on their pages – as much as two years ahead of the code’s publication.

Despite such warning it did not do much to tackle the issue.

In 2018, more than 400 pages were linked to the neo-Nazi group National Alliance, a white supremacist newspaper, the Irish Republican Army (IRA) and a neo-Nazi music group. The same analysis revealed that more than 35,000 Instagram and Twitter accounts were affiliated with the far right or included messages inciting violence.

The government has tried to tackle this problem. In 2011 it set up the Internet Forum to Combat Hate, Extremism and Antisemitism, but by 2017, its budget had shrunk to 9% of the overall state budget.

The government’s digital strategy for 2020-27, published in September 2019, also aims to tackle illegal online content. It says it won’t publish reports on the effectiveness of its “coordinated effort” to combat hate and terror online without its approval.

But its strategy does not include a strategy to tackle hate speech as such, nor the ability of state institutions to intervene after hate speech has become a crime.

This is because civil society campaigners point out that there is a gap in what companies can do to address social media content online – and what actions the state can take.

“The government isn’t ready to tackle online hate speech any more than it is ready