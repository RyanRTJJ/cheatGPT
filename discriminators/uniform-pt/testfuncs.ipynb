{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/detect/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datasets\n",
    "import transformers\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import functools\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "dataset = 'writing'\n",
    "dataset_key = 'document'\n",
    "n_samples = 100\n",
    "n_perturbation_list = '1,10,50'\n",
    "base_model_name = 'facebook/opt-2.7b'\n",
    "base_model_name_original = 'facebook/opt-2.7b'\n",
    "mask_filling_model_name = 't5-large'\n",
    "batch_size = 50\n",
    "do_top_k = False\n",
    "top_k = 40\n",
    "do_top_p = False\n",
    "top_p = 0.96\n",
    "output_name = 'n_perturb'\n",
    "openai_model = None\n",
    "openai_key = None\n",
    "mask_top_p = 1.0\n",
    "cache_dir = '~/.cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEPARATOR = '<<<SEP>>>'\n",
    "DATASETS = ['writing', 'english', 'german', 'pubmed']\n",
    "\n",
    "\n",
    "def load_pubmed(cache_dir):\n",
    "    data = datasets.load_dataset('pubmed_qa', 'pqa_labeled', split='train', cache_dir=cache_dir)\n",
    "    \n",
    "    # combine question and long_answer\n",
    "    data = [f'Question: {q} Answer:{SEPARATOR}{a}' for q, a in zip(data['question'], data['long_answer'])]\n",
    "\n",
    "    return data\n",
    "\n",
    "def process_prompt(prompt):\n",
    "    return prompt.replace('[ WP ]', '').replace('[ OT ]', '')\n",
    "\n",
    "def process_spaces(story):\n",
    "    return story.replace(\n",
    "        ' ,', ',').replace(\n",
    "        ' .', '.').replace(\n",
    "        ' ?', '?').replace(\n",
    "        ' !', '!').replace(\n",
    "        ' ;', ';').replace(\n",
    "        ' \\'', '\\'').replace(\n",
    "        ' â€™ ', '\\'').replace(\n",
    "        ' :', ':').replace(\n",
    "        '<newline>', '\\n').replace(\n",
    "        '`` ', '\"').replace(\n",
    "        ' \\'\\'', '\"').replace(\n",
    "        '\\'\\'', '\"').replace(\n",
    "        '.. ', '... ').replace(\n",
    "        ' )', ')').replace(\n",
    "        '( ', '(').replace(\n",
    "        ' n\\'t', 'n\\'t').replace(\n",
    "        ' i ', ' I ').replace(\n",
    "        ' i\\'', ' I\\'').replace(\n",
    "        '\\\\\\'', '\\'').replace(\n",
    "        '\\n ', '\\n').strip()\n",
    "\n",
    "def load_writing(cache_dir=None):\n",
    "    writing_path = 'data/writingPrompts'\n",
    "    \n",
    "    with open(f'{writing_path}/valid.wp_source', 'r') as f:\n",
    "        prompts = f.readlines()\n",
    "    with open(f'{writing_path}/valid.wp_target', 'r') as f:\n",
    "        stories = f.readlines()\n",
    "    \n",
    "    prompts = [process_prompt(prompt) for prompt in prompts]\n",
    "    joined = [process_spaces(prompt + \" \" + story) for prompt, story in zip(prompts, stories)]\n",
    "    filtered = [story for story in joined if 'nsfw' not in story and 'NSFW' not in story]\n",
    "\n",
    "    random.seed(0)\n",
    "    random.shuffle(filtered)\n",
    "\n",
    "    return filtered\n",
    "\n",
    "def load_language(language, cache_dir):\n",
    "    # load either the english or german portion of the wmt16 dataset\n",
    "    assert language in ['en', 'de']\n",
    "    d = datasets.load_dataset('wmt16', 'de-en', split='train', cache_dir=cache_dir)\n",
    "    docs = d['translation']\n",
    "    desired_language_docs = [d[language] for d in docs]\n",
    "    lens = [len(d.split()) for d in desired_language_docs]\n",
    "    sub = [d for d, l in zip(desired_language_docs, lens) if l > 100 and l < 150]\n",
    "    return sub\n",
    "\n",
    "def load_german(cache_dir):\n",
    "    return load_language('de', cache_dir)\n",
    "\n",
    "def load_english(cache_dir):\n",
    "    return load_language('en', cache_dir)\n",
    "\n",
    "def load(name, cache_dir, **kwargs):\n",
    "    if name in DATASETS:\n",
    "        load_fn = globals()[f'load_{name}']\n",
    "        return load_fn(cache_dir=cache_dir, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown dataset {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base_model(mask_model, base_model):\n",
    "    print('MOVING BASE MODEL TO GPU...', end='', flush=True)\n",
    "    start = time.time()\n",
    "    try:\n",
    "        mask_model.cpu()\n",
    "    except NameError:\n",
    "        pass\n",
    "    if openai_model is None:\n",
    "        base_model.to(DEVICE)\n",
    "    print(f'DONE ({time.time() - start:.2f}s)')\n",
    "\n",
    "def load_base_model_and_tokenizer(name):\n",
    "    if openai_model is None:\n",
    "        print(f'Loading BASE model {base_model_name}...')\n",
    "        base_model_kwargs = {}\n",
    "        if 'gpt-j' in name or 'neox' in name:\n",
    "            base_model_kwargs.update(dict(torch_dtype=torch.float16))\n",
    "        if 'gpt-j' in name:\n",
    "            base_model_kwargs.update(dict(revision='float16'))\n",
    "        print(name)\n",
    "        base_model = transformers.AutoModelForCausalLM.from_pretrained(name, **base_model_kwargs, cache_dir=cache_dir)\n",
    "    else:\n",
    "        base_model = None\n",
    "\n",
    "    optional_tok_kwargs = {}\n",
    "    if \"facebook/opt-\" in name:\n",
    "        print(\"Using non-fast tokenizer for OPT\")\n",
    "        optional_tok_kwargs['fast'] = False\n",
    "    if dataset in ['pubmed']:\n",
    "        optional_tok_kwargs['padding_side'] = 'left'\n",
    "    base_tokenizer = transformers.AutoTokenizer.from_pretrained(name, **optional_tok_kwargs, cache_dir=cache_dir)\n",
    "    base_tokenizer.pad_token_id = base_tokenizer.eos_token_id\n",
    "\n",
    "    return base_model, base_tokenizer\n",
    "\n",
    "# strip newlines from each example; replace one or more newlines with a single space\n",
    "def strip_newlines(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def generate_data(dataset, key, preproc_tokenizer):\n",
    "    # load data\n",
    "    if dataset in DATASETS:\n",
    "        data = load(dataset, cache_dir)\n",
    "    else:\n",
    "        data = datasets.load_dataset(dataset, split='train', cache_dir=cache_dir)[key]\n",
    "    \n",
    "\n",
    "    # get unique examples, strip whitespace, and remove newlines\n",
    "    # then take just the long examples, shuffle, take the first 5,000 to tokenize to save time\n",
    "    # then take just the examples that are <= 512 tokens (for the mask model)\n",
    "    # then generate n_samples samples\n",
    "\n",
    "    # remove duplicates from the data\n",
    "    data = list(dict.fromkeys(data))  # deterministic, as opposed to set()\n",
    "\n",
    "    # strip whitespace around each example\n",
    "    data = [x.strip() for x in data]\n",
    "\n",
    "    # remove newlines from each example\n",
    "    data = [strip_newlines(x) for x in data]\n",
    "\n",
    "    # try to keep only examples with > 250 words\n",
    "    if dataset in ['writing', 'squad', 'xsum']:\n",
    "        long_data = [x for x in data if len(x.split()) > 250]\n",
    "        if len(long_data) > 0:\n",
    "            data = long_data\n",
    "\n",
    "    random.seed(0)\n",
    "    random.shuffle(data)\n",
    "\n",
    "    data = data[:5_000]\n",
    "\n",
    "    # keep only examples with <= 512 tokens according to mask_tokenizer\n",
    "    # this step has the extra effect of removing examples with low-quality/garbage content\n",
    "    tokenized_data = preproc_tokenizer(data)\n",
    "    data = [x for x, y in zip(data, tokenized_data[\"input_ids\"]) if len(y) <= 512]\n",
    "\n",
    "    # print stats about remainining data\n",
    "    print(f\"Total number of samples: {len(data)}\")\n",
    "    print(f\"Average number of words: {np.mean([len(x.split()) for x in data])}\")\n",
    "\n",
    "    return data # generate_samples(data[:n_samples], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to absolute path: /home/raghav/cheatGPT/discriminators/uniform-pt/tmp_results/n_perturb/facebook_opt-2.7b-t5-large-temp/2023-03-08-03-29-51-866298-writing-100\n",
      "Using cache dir ~/.cache\n",
      "Loading BASE model facebook_opt-2.7b...\n",
      "facebook/opt-2.7b\n",
      "Using non-fast tokenizer for OPT\n",
      "Loading mask filling model t5-large...\n",
      "MOVING BASE MODEL TO GPU...DONE (2.97s)\n",
      "Loading dataset writing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (706 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 1011\n",
      "Average number of words: 309.6142433234421\n"
     ]
    }
   ],
   "source": [
    "API_TOKEN_COUNTER = 0\n",
    "\n",
    "if openai_model is not None:\n",
    "    import openai\n",
    "    assert openai_key is not None, \"Must provide OpenAI API key as --openai_key\"\n",
    "    openai.api_key = openai_key\n",
    "\n",
    "START_DATE = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "START_TIME = datetime.datetime.now().strftime('%H-%M-%S-%f')\n",
    "\n",
    "# define SAVE_FOLDER as the timestamp - base model name - mask filling model name\n",
    "# create it if it doesn't exist\n",
    "sampling_string = \"top_k\" if do_top_k else (\"top_p\" if do_top_p else \"temp\")\n",
    "output_subfolder = f\"{output_name}/\" if output_name else \"\"\n",
    "if openai_model is None:\n",
    "    base_model_name = base_model_name.replace('/', '_')\n",
    "else:\n",
    "    base_model_name = \"openai-\" + openai_model.replace('/', '_')\n",
    "SAVE_FOLDER = f\"tmp_results/{output_subfolder}{base_model_name}-{mask_filling_model_name}-{sampling_string}/{START_DATE}-{START_TIME}-{dataset}-{n_samples}\"\n",
    "if not os.path.exists(SAVE_FOLDER):\n",
    "    os.makedirs(SAVE_FOLDER)\n",
    "print(f\"Saving results to absolute path: {os.path.abspath(SAVE_FOLDER)}\")\n",
    "\n",
    "mask_filling_model_name = mask_filling_model_name\n",
    "n_samples = n_samples\n",
    "batch_size = batch_size\n",
    "n_perturbation_list = [int(x) for x in n_perturbation_list.split(\",\")]\n",
    "\n",
    "cache_dir = cache_dir\n",
    "os.environ[\"XDG_CACHE_HOME\"] = cache_dir\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "print(f\"Using cache dir {cache_dir}\")\n",
    "\n",
    "GPT2_TOKENIZER = transformers.GPT2Tokenizer.from_pretrained('gpt2', cache_dir=cache_dir)\n",
    "\n",
    "# generic generative model\n",
    "base_model, base_tokenizer = load_base_model_and_tokenizer(base_model_name_original)\n",
    "\n",
    "# mask filling t5 model\n",
    "print(f'Loading mask filling model {mask_filling_model_name}...')\n",
    "mask_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(mask_filling_model_name, cache_dir=cache_dir)\n",
    "try:\n",
    "    n_positions = mask_model.config.n_positions\n",
    "except AttributeError:\n",
    "    n_positions = 512\n",
    "\n",
    "preproc_tokenizer = transformers.AutoTokenizer.from_pretrained('t5-small', model_max_length=512, cache_dir=cache_dir)\n",
    "mask_tokenizer = transformers.AutoTokenizer.from_pretrained(mask_filling_model_name, model_max_length=n_positions, cache_dir=cache_dir)\n",
    "if dataset in ['english', 'german']:\n",
    "    preproc_tokenizer = mask_tokenizer\n",
    "\n",
    "load_base_model(mask_model, base_model)\n",
    "\n",
    "print(f'Loading dataset {dataset}...')\n",
    "data = generate_data(dataset, dataset_key, preproc_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Aliens visit earth and are fascinated by other animals but find humans completely unremarkable. Hello, let me begin this report by stating just how thankful I am to have been sent to Earth. It was truly an eye opening experience and without your guidance and funding would have been utterly impossible. Let me give you an overview of how Earth functions, and I think you will be surprised to find that it works very similar to our planet. There exists a large number of organisms going through one stage or another of change while they fall into their respective places on the planet. The conclusion of my research has confirmed what we have believed to be a universal law called net impact theory, which can be described in two parts: 1. a species can only exist if it has a net positive impact on its environment. 2. Species' which detract from their environment will go extinct. My research will hopefully continue to discredit the theory which this universal law replaced - survival of the fittest. Earth has many thriving organisms, many plants and animals which have been around since close to the beginning of its existence. I found the fern particularly interesting, its large external structures providing cover for tree saplings, allowing for the growth of massive rain forests across the planet. I encountered very few organisms which seemed to fall into part two of net impact theory, which indicates that Earth will continue to thrive as it evolves into a more perfect system. The organisms I cataloged as most likely to fit part two were the mosquito, the panda and the human, and I am curious to see upon my next journey to Earth if these vermin will still be present on the planet or if net impact will take its course. Now that this formality is out of the way, let me get on to the exciting description I was able to craft of the earth worm....\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data))\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
